{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PolyCollection\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import scienceplots\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "REPO_PATH = os.getenv(\"REPO_PATH\")\n",
    "sys.path.insert(0, rf'{REPO_PATH}src')\n",
    "\n",
    "from utils.main_utils import combload_topic_dfs, apply_nb_style\n",
    "from utils.eval_utils import describe_df\n",
    "\n",
    "apply_nb_style()\n",
    "\n",
    "plt.style.use('science')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = ['CRU', 'CWP', 'CEN']\n",
    "\n",
    "FUTURES = ['CLc1', 'LCOc1']\n",
    "\n",
    "SENTIMENT_COLUMNS = [\n",
    "    'TextBlob_fullStory', \n",
    "    'VADER_fullStory'\n",
    "]\n",
    "\n",
    "RESAMPLE_WINDOW = '5min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df: pd.DataFrame = combload_topic_dfs(\n",
    "    TOPICS, \n",
    "    lambda topic: rf'{REPO_PATH}data\\sentiment_data\\{topic}_ARTICLE_SENTIMENT.csv',\n",
    "    include_topic=True\n",
    ")\n",
    "news_df.index = pd.to_datetime(news_df.index)\n",
    "\n",
    "news_df['subtopic'] = news_df['topic'] + '_' + news_df['LDA_topic'].astype(str)\n",
    "news_df['crosstopic'] = 'CT_' + news_df['cross_topic'].astype(str)\n",
    "\n",
    "news_df.drop(\n",
    "    columns=[col for col in news_df.columns if 'headline' in col] + ['LDA_topic', 'cross_topic'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "f_dfs: dict[str, pd.DataFrame] = {\n",
    "    future: pd.read_csv(\n",
    "        rf'{REPO_PATH}data\\raw_futures_data\\{future}_High_Frequency.csv', \n",
    "        index_col=0\n",
    "    ) for future in FUTURES\n",
    "}\n",
    "\n",
    "display(news_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample and add measures to futures data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNUALIZATION_FACTOR = np.sqrt(276 * 252)\n",
    "\n",
    "for df in f_dfs.values():\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['LOGRET'] = np.log(df['CLOSE']).diff()\n",
    "    df.loc[df.index[0], 'LOGRET'] = 0\n",
    "\n",
    "fr_dfs: dict[str, pd.DataFrame] = {\n",
    "    future: pd.DataFrame(\n",
    "        {\n",
    "            'CLOSE': df['CLOSE'].resample(RESAMPLE_WINDOW).last(),\n",
    "            'VOLUME': df['VOLUME'].resample(RESAMPLE_WINDOW).sum(),\n",
    "            'COUNT': df['COUNT'].resample(RESAMPLE_WINDOW).sum(),\n",
    "            'LOGRET': df['LOGRET'].resample(RESAMPLE_WINDOW).sum(),\n",
    "            'REALIZED_VOL': np.sqrt((df['LOGRET'] ** 2).resample(RESAMPLE_WINDOW).sum())\n",
    "        }\n",
    "    ) for future, df in f_dfs.items()\n",
    "}\n",
    "\n",
    "for i, (key, df) in enumerate(fr_dfs.items()):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['REALIZED_VOL'] = df['REALIZED_VOL'] * ANNUALIZATION_FACTOR\n",
    "\n",
    "log_list = []\n",
    "rev_list = []\n",
    "\n",
    "for i, (key, df) in enumerate(fr_dfs.items()):\n",
    "    log_df = describe_df(df['LOGRET'].resample('1D').sum() * 100)\n",
    "    log_list.append(log_df.T)\n",
    "    \n",
    "    rev_df = describe_df(df['REALIZED_VOL'])\n",
    "    rev_list.append(rev_df.T)\n",
    "\n",
    "desc_df = pd.concat(\n",
    "    [\n",
    "        pd.concat(log_list, axis=1, keys=fr_dfs.keys()),\n",
    "        pd.concat(rev_list, axis=1, keys=fr_dfs.keys())\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "display(desc_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=200)\n",
    "\n",
    "colors = sns.color_palette('twilight', 2)\n",
    "\n",
    "plot_content = {\n",
    "    'Daily': np.sqrt((f_dfs['CLc1']['LOGRET'] ** 2).resample('1d').sum()),\n",
    "    '5min': fr_dfs['CLc1']['REALIZED_VOL']\n",
    "}\n",
    "\n",
    "for i, (key, df) in enumerate(plot_content.items()):\n",
    "    plot_acf(\n",
    "        df,\n",
    "        ax=axs[i], \n",
    "        color=colors[i],\n",
    "        lags=20,\n",
    "        vlines_kwargs={'colors': colors[i]}\n",
    "    )\n",
    "    for item in axs[i].collections:\n",
    "        if type(item)==PolyCollection:\n",
    "            item.set_facecolor(colors[i])\n",
    "\n",
    "    axs[i].set_title(f'{key} Realized Volatility ACF', fontsize=16)\n",
    "    axs[i].set_xlabel('Lags', fontsize=16)\n",
    "    axs[i].set_ylabel('Correlation', fontsize=16)\n",
    "    axs[i].grid(alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/acf_realized_volatility.png')\n",
    "\n",
    "print(f'Daily ACF(1): {plot_content[\"Daily\"].autocorr(1):.4f}')\n",
    "print(f'5min  ACF(1): {plot_content[\"5min\"].autocorr(1):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample and treat news sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'topic': TOPICS,\n",
    "    'subtopic': news_df['subtopic'].unique(),\n",
    "    'crosstopic': news_df['crosstopic'].unique()\n",
    "}\n",
    "\n",
    "resample_dfs = []\n",
    "for key, classifier in classifiers.items():\n",
    "    for topic in classifier:\n",
    "        resampled_df = pd.DataFrame(\n",
    "            {\n",
    "                col: news_df[col][news_df[key] == topic].resample(RESAMPLE_WINDOW).mean()\n",
    "                for col in SENTIMENT_COLUMNS\n",
    "            }\n",
    "        ).fillna(0).add_prefix(f'{topic}_')\n",
    "        resample_dfs.append(resampled_df)\n",
    "\n",
    "resample_df = pd.concat(resample_dfs, axis=1).fillna(0)\n",
    "resample_df.columns = resample_df.columns.str.replace('_fullStory', '')\n",
    "\n",
    "# combine news and futures data\n",
    "combined_dfs: dict[str, pd.DataFrame] = {\n",
    "    future: resample_df.fillna(0).join(fr_dfs[future]).dropna()\n",
    "    for future in FUTURES\n",
    "}\n",
    "\n",
    "for df in combined_dfs.values():\n",
    "    display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY = 288 # number of 5-minute intervals\n",
    "\n",
    "dfs = combined_dfs.copy()\n",
    "for key, df in dfs.items():\n",
    "    tqdm.pandas(desc=f'Calculating Topic SI for {key}')\n",
    "    for col_name in tqdm(resample_df.columns):\n",
    "        df[f'{col_name}_SI'] = df[col_name].ewm(span=DECAY).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add temporal features and save dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal(df: pd.DataFrame) -> None:\n",
    "    df['HOUR'] = df.index.hour\n",
    "    df['MINUTE'] = df.index.minute\n",
    "    df['DAY_OF_WEEK'] = df.index.dayofweek\n",
    "    df['DAY_OF_MONTH'] = df.index.day\n",
    "    df['MONTH'] = df.index.month\n",
    "    df['YEAR'] = df.index.year\n",
    "\n",
    "def add_lags(df, column, lags: int) -> None:\n",
    "    for lag in range(lags):\n",
    "        df[f'RV_LAG_{lag + 1}'] = df[column].shift(lag + 1)\n",
    "\n",
    "def add_target(df, column, horizon: int) -> None:\n",
    "    for lag in range(horizon):\n",
    "        df[f'TARGET_{lag + 1}'] = df[column].shift(-lag - 1)\n",
    "\n",
    "for future in FUTURES:\n",
    "    df = combined_dfs[future]\n",
    "    add_temporal(df)\n",
    "    add_lags(df, 'REALIZED_VOL', 5)\n",
    "    \n",
    "    add_target(df, 'REALIZED_VOL', 1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df.to_csv(\n",
    "        rf'{REPO_PATH}data\\prepared_data\\{future}_{RESAMPLE_WINDOW}_resampled.csv'\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
