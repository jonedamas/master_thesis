{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from BERT_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_headlines = pd.read_csv(r'C:\\Users\\joneh\\master_thesis\\data\\news\\TheGuardian\\TG_CrudeANDOil.csv')\n",
    "df_headlines['datetime'] = pd.to_datetime(df_headlines['datetime']).dt.date\n",
    "df_headlines = df_headlines.drop(columns=['id', 'webPublicationDate', 'type', 'sectionId', 'sectionName', 'webUrl', 'apiUrl', 'isHosted', 'pillarId', 'pillarName'])\n",
    "\n",
    "df_prices = pd.read_csv(r'C:\\Users\\joneh\\master_thesis\\data\\time_series\\YahooFinance\\CL=F_20years.csv')\n",
    "df_prices.index = pd.to_datetime(df_prices['Date']).dt.date\n",
    "df_prices = df_prices.drop(columns=['Date', 'Open', 'High', 'Low', 'Close'])\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels = 3)\n",
    "\n",
    "# Load the pre-trained model's tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and add GARCH(1,1) volatility labels to headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model = arch_model(df_prices['Adj Close'], vol='Garch', p=1, q=1, dist='Normal')\n",
    "train_fit = garch_model.fit(disp='off')\n",
    "\n",
    "# add garch volatility data to df_prices\n",
    "df_prices['garch_vol'] = train_fit.conditional_volatility\n",
    "df_prices['log_vol'] = np.log(df_prices['garch_vol']).diff()\n",
    "df_prices['log_ret'] = np.log(df_prices['Adj Close']).diff()\n",
    "\n",
    "# add garch_volatility and log_vol labels to df_headlines\n",
    "\n",
    "for label in ['garch_vol', 'log_vol', 'log_ret', 'Adj Close', 'Volume']:\n",
    "    df_headlines[label] = df_headlines['datetime'].map(df_prices[label])\n",
    "\n",
    "# fill NaN values\n",
    "df_headlines['garch_vol'].fillna(method='ffill', inplace=True)\n",
    "df_headlines['log_vol'].fillna(method='bfill', inplace=True)\n",
    "df_headlines['log_ret'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "df_headlines.dropna(inplace=True)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "res = {}\n",
    "\n",
    "for i, row in tqdm(df_headlines.iterrows(), total=len(df_headlines)):\n",
    "    text = row['webTitle']\n",
    "    myid = i\n",
    "    res[myid] = sia.polarity_scores(text)\n",
    "\n",
    "df_headlines['sia'] = pd.DataFrame(res).T['compound']\n",
    "\n",
    "# df_headlines['FinBERT'] = get_BERT_sentiment_per_headline(\n",
    "#     df_headlines['webTitle'], \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "display(df_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label = 'sia'\n",
    "y_label = 'Volume'\n",
    "\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_headlines[x_label], df_headlines[y_label])\n",
    "print(f'p-value: {p_value:.7f}')\n",
    "print(r_value)\n",
    "print(slope)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "df_headlines.plot(kind='scatter', x=x_label, y=y_label, s=2, ax=ax)\n",
    "ax.plot(df_headlines[x_label], slope * df_headlines[x_label] + intercept, color='red')\n",
    "ax.grid(alpha=0.2)\n",
    "ax.set_title(f'{x_label} vs {y_label}')\n",
    "\n",
    "\n",
    "# correlation\n",
    "print(f'Correlation: {df_headlines[x_label].corr(df_headlines[y_label])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize pre-trained model\n",
    "\n",
    "FinBERT is initialized as a pre-trained model. The model is trained on the financial domain and is fine-tuned on the financial sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels = 3)\n",
    "\n",
    "# Load the pre-trained model's tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_sentences = df_headlines['webTitle'].tolist()\n",
    "\n",
    "# Tokenize your dataset\n",
    "inputs = tokenizer(batch_of_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "display(inputs)\n",
    "\n",
    "# # Create a dataloader\n",
    "# train_dataloader = DataLoader(training_dataset, batch_size=32)\n",
    "\n",
    "# # Prepare optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# num_epochs = 5\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['labels']\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Save your model\n",
    "# model.save_pretrained('models/tuned_BERT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
