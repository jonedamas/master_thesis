{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "import ipywidgets as widgets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "pyLDAvis.enable_notebook()\n",
    "REPO_PATH =  os.getenv('REPO_PATH')\n",
    "\n",
    "sys.path.insert(0, rf'{REPO_PATH}src_HF')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = 'CRU'\n",
    "\n",
    "text_df = pd.read_json(rf'{REPO_PATH}data\\news_data\\EIKON_{TOPIC}_NEWS_COMPLETE.json', lines=True, orient='records')\n",
    "\n",
    "_, text_df['cleaned_tokenized'] = clean_token_series(text_df['fullStory'])\n",
    "\n",
    "display(text_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS: int = 4\n",
    "CHUNKSIZE: int = 500\n",
    "PASSES: int = 20\n",
    "ITERATIONS: int = 400\n",
    "EVAL_EVERY: int = 1\n",
    "\n",
    "doc_list: list = text_df['cleaned_tokenized'].to_list()\n",
    "\n",
    "stop_words: set[str] =  IGNORE_WORDS\n",
    "\n",
    "# remove stop words\n",
    "doc_list = [[word for word in doc if word not in stop_words] for doc in doc_list]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "dictionary.id2token = {id: token for token, id in dictionary.token2id.items()}\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=ITERATIONS,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=PASSES,\n",
    "    eval_every=EVAL_EVERY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.show_topics(formatted=False, num_topics=NUM_TOPICS, num_words=15)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    data = {\n",
    "        'word': [word for word, _ in topic[1]],\n",
    "        'weight': [weight for _, weight in topic[1]]\n",
    "    }\n",
    "    dataframes[f'Topic {i}'] = pd.DataFrame(data)\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list(dataframes.keys()),\n",
    "    value=dataframes.keys(),\n",
    "    description='Select DataFrame:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display the DataFrame\n",
    "def display_dataframe(change):\n",
    "    display(dataframes[change['new']])\n",
    "\n",
    "# Call display_dataframe function when the dropdown value changes\n",
    "dropdown.observe(display_dataframe, names='value')\n",
    "\n",
    "# Show the dropdown\n",
    "display(dropdown)\n",
    "\n",
    "# Initially display the first DataFrame\n",
    "display(dataframes[dropdown.value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDAvis visualization of gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(\n",
    "    topic_model = model, \n",
    "    corpus = corpus, \n",
    "    dictionary = dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign topics to each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the article\n",
    "def classify_article(article, model):\n",
    "    bow = dictionary.doc2bow(article['cleaned_tokenized'])\n",
    "    topic_distribution = model.get_document_topics(bow)\n",
    "    topic_nr = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    return topic_nr\n",
    "\n",
    "text_df['topic'] = text_df.apply(lambda x: classify_article(x, model), axis=1)\n",
    "\n",
    "topic_dict = dict(zip(text_df['storyId'], text_df['topic']))\n",
    "\n",
    "with open(repo_path + rf'data\\topics\\{eikon_topic}_TOPICS.json', 'w') as f:\n",
    "    json.dump(topic_dict, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
