{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "pyLDAvis.enable_notebook()\n",
    "repo_path =  os.getenv('REPO_PATH')\n",
    "\n",
    "sys.path.insert(0, repo_path + r'src_HF')\n",
    "from utils.main_utils import *\n",
    "from utils.text_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eikon_topic = 'CRU'\n",
    "\n",
    "text_df = pd.read_json(repo_path + rf'data\\news_data\\EIKON_{eikon_topic}_NEWS_COMPLETE.json', lines=True, orient='records')\n",
    "\n",
    "_, text_df['cleaned_tokenized'] = clean_token_series(text_df['fullStory'])\n",
    "\n",
    "display(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics: int = 10\n",
    "chunksize: int = 500\n",
    "passes: int = 20\n",
    "iterations: int = 400\n",
    "eval_every: int = 1\n",
    "\n",
    "doc_list: list = text_df['cleaned_tokenized'].to_list()\n",
    "\n",
    "stop_words =  ignore_words\n",
    "\n",
    "# remove stop words\n",
    "doc_list = [[word for word in doc if word not in stop_words] for doc in doc_list]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "dictionary.id2token = {id: token for token, id in dictionary.token2id.items()}\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.show_topics(formatted=False)\n",
    "\n",
    "for topic in topics:\n",
    "    df = pd.DataFrame(topic[1], columns=['word', 'probability'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign topics to each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the article\n",
    "def classify_article(article, model):\n",
    "    bow = dictionary.doc2bow(article['cleaned_tokenized'])\n",
    "    topic_distribution = model.get_document_topics(bow)\n",
    "    topic_nr = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    return topic_nr\n",
    "\n",
    "text_df['topic'] = text_df.apply(lambda x: classify_article(x, model), axis=1)\n",
    "\n",
    "topic_dict = dict(zip(text_df['storyId'], text_df['topic']))\n",
    "\n",
    "with open(repo_path + rf'data\\topics\\{eikon_topic}_TOPICS.json', 'w') as f:\n",
    "    json.dump(topic_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
