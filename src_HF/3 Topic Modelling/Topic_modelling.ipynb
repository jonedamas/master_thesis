{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "pyLDAvis.enable_notebook()\n",
    "REPO_PATH =  os.getenv('REPO_PATH')\n",
    "\n",
    "sys.path.insert(0, rf'{REPO_PATH}src_HF')\n",
    "from utils.text_utils import clean_token_series, IGNORE_WORDS\n",
    "from utils.topic_utils import classify_article, LDAModelSetup\n",
    "from utils.main_utils import combload_topic_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS: list[str] = ['CRU', 'CWP', 'CEN']\n",
    "\n",
    "text_df = combload_topic_dfs(\n",
    "    TOPICS, \n",
    "    lambda topic: rf'{REPO_PATH}data\\news_data\\EIKON_{topic}_NEWS_COMPLETE.json'\n",
    ")\n",
    "\n",
    "text_df['cleaned_tokenized'] = clean_token_series(text_df['fullStory'])\n",
    "\n",
    "display(text_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_params: dict[str, int] = {\n",
    "    'num_topics': 3,\n",
    "    'chunksize': 500,\n",
    "    'passes': 20,\n",
    "    'iterations': 100,\n",
    "    'eval_every': 1\n",
    "}\n",
    "\n",
    "tokenized_series = text_df[text_df['topic'] == 'CRU']['cleaned_tokenized']\n",
    "\n",
    "doc_list: list = tokenized_series.to_list()\n",
    "\n",
    "stop_words: set[str] =  IGNORE_WORDS\n",
    "\n",
    "# addnumbers to stop words\n",
    "stop_words.update({str(i) for i in range(3000)})\n",
    "\n",
    "# remove stop words\n",
    "doc_list = [[word for word in doc if word not in stop_words] for doc in doc_list]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "dictionary.id2token = {id: token for token, id in dictionary.token2id.items()}\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    eta='auto',\n",
    "    workers=6,\n",
    "    **lda_params\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) model setup for subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_params: dict[str, int] = {\n",
    "    'num_topics': 3,\n",
    "    'chunksize': 500,\n",
    "    'passes': 20,\n",
    "    'iterations': 100,\n",
    "    'eval_every': 1\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for topic in TOPICS:\n",
    "    model = LDAModelSetup(\n",
    "        text_df.loc[text_df['topic'] == topic, 'cleaned_tokenized'],\n",
    "        name=topic,\n",
    "        stopwords=IGNORE_WORDS, \n",
    "        lda_params=lda_params\n",
    "    )\n",
    "\n",
    "    models[topic] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in models.items():\n",
    "    print(f'Creating model for {key}...')\n",
    "    model.generate_model()\n",
    "    model.generate_pyLDAvis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDAvis visualization of gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = 'CEN'\n",
    "\n",
    "display(models[TOPIC].visfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), dpi=200)\n",
    "\n",
    "for i, topic in enumerate(TOPICS):\n",
    "    models[topic].plot_pyLDAvis(axs[i])\n",
    "\n",
    "fig.tight_layout(pad=1)\n",
    "\n",
    "fig.savefig(rf'images\\pyLDAvis_topic_PC.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA setup for cross-topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_params: dict[str, int] = {\n",
    "    'num_topics': 5,\n",
    "    'chunksize': 500,\n",
    "    'passes': 20,\n",
    "    'iterations': 500,\n",
    "    'eval_every': 1\n",
    "}\n",
    "\n",
    "full_model = LDAModelSetup(\n",
    "    text_df['cleaned_tokenized'],\n",
    "    name='All topics',\n",
    "    stopwords=IGNORE_WORDS, \n",
    "    lda_params=lda_params\n",
    ")\n",
    "\n",
    "full_model.generate_model()\n",
    "full_model.generate_pyLDAvis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_model.visfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_df.copy()\n",
    "df['crosstopic'] = df.apply(\n",
    "    lambda x: classify_article(\n",
    "        x, \n",
    "        full_model.dictionary, \n",
    "        full_model.model\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 3), dpi=200)\n",
    "\n",
    "colors = sns.color_palette('twilight', n_colors=3)\n",
    "order = []\n",
    "\n",
    "for topic in range(len(df['crosstopic'].unique())):\n",
    "    values = df[df['crosstopic'] == topic]['topic'].value_counts().reindex(TOPICS)\n",
    "    values.plot.pie(ax=ax[topic], colors=colors)\n",
    "\n",
    "    ax[topic].set_ylabel('')\n",
    "    ax[topic].set_title(f'Topic {topic + 1}')\n",
    "\n",
    "    # print top 10 words for each topic\n",
    "    top_words = full_model.model.show_topic(topic, topn=10)\n",
    "    words = [word for word, _ in top_words]\n",
    "    order.append(words)\n",
    "    print(f'Topic {topic + 1}: {words}')\n",
    "\n",
    "fig.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA visualization of LDA model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7), dpi=200)\n",
    "\n",
    "full_model.plot_pyLDAvis(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign topics to each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in TOPICS:\n",
    "    df = text_df[text_df['topic'] == topic]\n",
    "    df['topic'] = df.apply(\n",
    "        lambda x: classify_article(\n",
    "            x, \n",
    "            models[topic].dictionary, \n",
    "            models[topic].model\n",
    "        ), axis=1\n",
    "    )\n",
    "    topic_dict = dict(zip(df['storyId'], df['topic']))\n",
    "\n",
    "    with open(rf'{REPO_PATH}data\\topics\\{topic}_TOPICS.json', 'w') as f:\n",
    "        json.dump(topic_dict, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
