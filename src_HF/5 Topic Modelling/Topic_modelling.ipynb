{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "repo_path =  os.getenv('REPO_PATH')\n",
    "\n",
    "sys.path.insert(0, repo_path + r'src_HF')\n",
    "from utils.main_utils import *\n",
    "from utils.text_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_json(repo_path + r'data\\news_data\\EIKON_CRU_NEWS_COMPLETE.json', lines=True, orient='records')\n",
    "\n",
    "_, text_df['cleaned_tokenized'] = clean_token_series(text_df['fullStory'])\n",
    "\n",
    "display(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(text_df['cleaned_tokenized'].to_list())\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_df['cleaned_tokenized'].to_list()]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a dataframe with the top word for each topic\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "\n",
    "for topic_number in range(10):\n",
    "    topic = topics[topic_number]\n",
    "    print(topic[0])\n",
    "    print(topic[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in tqdm(enumerate(model.components_)):\n",
    "\n",
    "        # create dataframe\n",
    "        df = pd.DataFrame(topic, vectorizer.get_feature_names_out(), columns=[\"score\"])\n",
    "\n",
    "        # sort by score\n",
    "        df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "        # display top n words\n",
    "        display(Markdown(f\"### Topic {idx}\"))\n",
    "        display(df.head(8))\n",
    "\n",
    "# Print the topics\n",
    "print_topics(ldamodel, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
