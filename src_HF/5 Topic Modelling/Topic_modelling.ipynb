{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, r'c:\\Users\\joneh\\master_thesis\\src_HF')\n",
    "from utils.main_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(r'C:\\Users\\joneh\\master_thesis\\data\\raw_news_stories\\EIKON_CRU_NEWS_FULL.csv')\n",
    "# remove all rows where fullStory is 'error'\n",
    "text_df = text_df[text_df['fullStory'] != 'error']\n",
    "# remove all rows where fullStory is float\n",
    "text_df = text_df[text_df['fullStory'].apply(lambda x: isinstance(x, str))]\n",
    "# reset index\n",
    "text_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str: str = text_df['fullStory'][555]\n",
    "display(text_str)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens: list = word_tokenize(text_str)\n",
    "print('Tokens:', len(tokens))\n",
    "\n",
    "# Remove punctuation\n",
    "tokens_wo_punct = [word for word in tokens if word.isalnum()]\n",
    "print('Tokens without punctuation:', len(tokens_wo_punct))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_wo_sw = [word for word in tokens_wo_punct if word.lower() not in stop_words]\n",
    "display(tokens_wo_sw)\n",
    "print('Tokens without stopwords:', len(tokens_wo_sw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "other_words = set(\n",
    "    [\n",
    "        'Full', 'Story', 'Reuters', 'copyright', 'c', 'Thomson', 'Click', 'Restrictions',\n",
    "        'Thomson Reuters', 'Full Story', 'Click Restrictions', 'c Copyright', 'Copyright Thomson',\n",
    "        'Restrictions https', 'Reuters Click', 'Final Terms'\n",
    "    ]\n",
    ")\n",
    "stop_words = stop_words.union(other_words)\n",
    "\n",
    "def clean_tokens(tokens: list[str]) -> list[str]:\n",
    "    tokens_wo_punct: list = [word for word in tokens if word.isalnum()]\n",
    "    tokens_wo_sw: list = [word for word in tokens_wo_punct if word.lower() not in stop_words]\n",
    "    return tokens_wo_sw\n",
    "\n",
    "def clean_series(series: pd.Series) -> pd.DataFrame:\n",
    "    tokenized: pd.Series = series.apply(word_tokenize)\n",
    "    cleaned: pd.Series = tokenized.apply(clean_tokens)\n",
    "    return tokenized, cleaned\n",
    "\n",
    "text_df['tokenized'], text_df['cleaned'] = clean_series(text_df['fullStory'])\n",
    "\n",
    "display(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "Wordcloud = WordCloud(\n",
    "    width=800, \n",
    "    height=400,\n",
    "    max_font_size=100,\n",
    "    colormap='twilight',\n",
    "    background_color='white'\n",
    ").generate(' '.join(text_df['cleaned'].sum()))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4), facecolor=None)\n",
    "plt.imshow(Wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "fig.savefig(r'C:\\Users\\joneh\\master_thesis\\src_HF\\5 Topic Modelling\\wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(text_df['cleaned'].to_list())\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_df['cleaned'].to_list()]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a dataframe with the top word for each topic\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "\n",
    "for topic_number in range(10):\n",
    "    topic = topics[topic_number]\n",
    "    print(topic[0])\n",
    "    print(topic[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in tqdm(enumerate(model.components_)):\n",
    "\n",
    "        # create dataframe\n",
    "        df = pd.DataFrame(topic, vectorizer.get_feature_names_out(), columns=[\"score\"])\n",
    "\n",
    "        # sort by score\n",
    "        df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "        # display top n words\n",
    "        display(Markdown(f\"### Topic {idx}\"))\n",
    "        display(df.head(8))\n",
    "\n",
    "# Print the topics\n",
    "print_topics(ldamodel, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
